"""Stage input processor for Qwen3-TTS: Talker -> Code2Wav."""

from typing import Any

import torch
from vllm.logger import init_logger

logger = init_logger(__name__)


def _extract_last_frame(pooling_output: dict[str, Any]) -> torch.Tensor | None:
    audio_codes = pooling_output.get("audio_codes")
    if not isinstance(audio_codes, torch.Tensor) or audio_codes.numel() == 0:
        return None
    if audio_codes.ndim == 2:
        frame = audio_codes[-1]
        if frame.numel() == 0 or not bool(frame.any().item()):
            return None
        return frame.to(torch.long).reshape(-1)
    if audio_codes.ndim == 1:
        return audio_codes.to(torch.long).reshape(-1)
    raise ValueError(f"Invalid audio_codes shape for Qwen3-TTS async_chunk: {tuple(audio_codes.shape)}")


def talker2code2wav_async_chunk(
    transfer_manager: Any,
    pooling_output: dict[str, Any] | None,
    request: Any,
    is_finished: bool = False,
) -> dict[str, Any] | None:
    request_id = request.external_req_id
    finished = bool(is_finished or request.is_finished())

    if isinstance(pooling_output, dict):
        frame = _extract_last_frame(pooling_output)
        if frame is not None:
            codec_codes = frame.cpu().tolist()
            transfer_manager.code_prompt_token_ids[request_id].append(codec_codes)
    elif not finished:
        # Some steps may not produce pooling_output. Only flush on finish.
        return None

    connector = getattr(transfer_manager, "connector", None)
    raw_cfg = getattr(connector, "config", {}) or {}
    cfg = raw_cfg.get("extra", raw_cfg) if isinstance(raw_cfg, dict) else {}
    chunk_size = int(cfg.get("codec_chunk_frames", 25))
    left_context_size = int(cfg.get("codec_left_context_frames", 25))
    initial_chunk_size = int(cfg.get("initial_codec_chunk_frames", 0))
    if chunk_size <= 0 or left_context_size < 0:
        raise ValueError(
            f"Invalid codec chunk config: codec_chunk_frames={chunk_size}, "
            f"codec_left_context_frames={left_context_size}"
        )
    if initial_chunk_size >= chunk_size:
        if initial_chunk_size > 0:
            logger.warning(
                "initial_codec_chunk_frames=%d >= codec_chunk_frames=%d, clamping to codec_chunk_frames.",
                initial_chunk_size,
                chunk_size,
            )
        initial_chunk_size = chunk_size
    length = len(transfer_manager.code_prompt_token_ids[request_id])

    # Avoid emitting empty chunks during normal streaming. If the request is
    # finished and nothing was produced, emit an EOF marker.
    if length <= 0:
        if finished:
            return {
                "code_predictor_codes": [],
                "finished": torch.tensor(True, dtype=torch.bool),
            }
        return None

    in_warmup = initial_chunk_size > 0 and length <= chunk_size

    if in_warmup:
        # Warmup phase: emit every initial_chunk_size frames with full context.
        # Track frames already delivered using put_req_chunk counter.
        already_sent = transfer_manager.put_req_chunk[request_id] * initial_chunk_size
        pending = length - already_sent
        at_initial_boundary = pending >= initial_chunk_size
        at_chunk_boundary = length >= chunk_size
        if not at_initial_boundary and not at_chunk_boundary and not finished:
            return None
        # At chunk_size boundary, flush remaining even if < initial_chunk_size.
        context_length = min(pending, initial_chunk_size)
        if at_chunk_boundary and not at_initial_boundary:
            context_length = pending
        end_index = length
        ctx_frames = max(0, length - context_length)
        window_frames = transfer_manager.code_prompt_token_ids[request_id][:length]
    else:
        # Normal phase: standard chunk_size cadence with left_context sliding window.
        adjusted = (length - chunk_size) if initial_chunk_size > 0 else length
        chunk_length = adjusted % chunk_size
        if chunk_length != 0 and not finished:
            return None
        context_length = chunk_length if chunk_length != 0 else chunk_size
        end_index = min(length, left_context_size + context_length)
        ctx_frames = max(0, int(end_index - context_length))
        window_frames = transfer_manager.code_prompt_token_ids[request_id][-end_index:]

    # Pack context + chunk into codebook-major flat codes for adapter.
    code_predictor_codes = torch.tensor(window_frames).transpose(0, 1).reshape(-1).tolist()

    # Build final prompt_token_ids with ctx_frames header for Qwen3-TTS Code2Wav.
    # The model expects input_ids layout: [ctx_frames, *flat_codes].
    return {
        "code_predictor_codes": [int(ctx_frames)] + code_predictor_codes,
        "finished": torch.tensor(finished, dtype=torch.bool),
    }
